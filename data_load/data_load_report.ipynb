{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241f04f-873e-4e22-bb59-57ee7abbd596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy --target ./python\n",
    "!pip install --upgrade numexpr --target ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f39c2-1b3c-40b1-a1a0-9239ab65316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from model import *\n",
    "\n",
    "#根据时间情况修改index和language值\n",
    "index =  \"finance_annual_report_demo_1031\"\n",
    "embedding_endpoint_name = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "embedding_type = 'bedrock' if embedding_endpoint_name.find('titan') or embedding_endpoint_name.find('cohere') else 'sagemaker'\n",
    "embeddings = init_embeddings_bedrock(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f25c55-868a-483b-ad3e-329ab2d57906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import base64\n",
    "from opensearch_multimodel_dataload import add_multimodel_documents\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "\n",
    "# model_name = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "model_name = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "# model_name = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# model_name = \"us.meta.llama3-2-90b-instruct-v1:0\"\n",
    "\n",
    "llm = init_model_bedrock(model_name)\n",
    "\n",
    "text_max_length = 300\n",
    "llm_max_size = 1000\n",
    "\n",
    "def is_json(myjson):\n",
    "    try:\n",
    "        json.loads(myjson)\n",
    "    except ValueError as e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a document manager at an financial company and your task is to extract useful information from document images.\n",
    "<instructions>\n",
    "1.don't make up content.keep all the content in the documents\n",
    "2.No preface, just output the document content directly.\n",
    "3.Output the document in markdown format, and keep the rows and columns aligned for the table.\n",
    "4.summarize page content to facilitate searching, output the summarize content in<summarize></summarize> tag after page content\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "files_path = '../docs/finance_annual_report/'\n",
    "\n",
    "files = os.listdir(files_path)\n",
    "for file in files:\n",
    "    file_path = files_path + file\n",
    "    print(file_path)\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    previous_page_content = ''\n",
    "    \n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    images = []\n",
    "    \n",
    "    for i in tqdm(range(doc.page_count)):\n",
    "\n",
    "        if i < 90 or i > 130:\n",
    "            continue\n",
    "        time.sleep(60)\n",
    "        print('i:',i)\n",
    "        page = doc.load_page(i)\n",
    "        pix = page.get_pixmap(dpi=150)\n",
    "\n",
    "        imgb64 = base64.b64encode(pix.tobytes()).decode(\"utf-8\")\n",
    "        model_kwargs = {'image': imgb64,'image_type':'jpeg','max_tokens':4096}\n",
    "\n",
    "        llm.model_kwargs = model_kwargs\n",
    "        response = llm(prompt)\n",
    "        response_list = response.split('<summarize>')\n",
    "        current_page = response_list[0].replace('Table of Contents','').replace('```','')[:-4].strip()\n",
    "        current_summarize = response_list[1].replace('</summarize>','').replace('```','').strip()\n",
    "        two_page_content = previous_page_content + ' ' +current_page\n",
    "        previous_page_content = current_page\n",
    "\n",
    "        # print('two_page_content:',two_page_content)\n",
    "        # print('*********')\n",
    "\n",
    "\n",
    "        content_set = set()\n",
    "        \n",
    "        # split and save summarize content\n",
    "        if len(current_summarize) > text_max_length:\n",
    "            summarize_sentences = current_summarize.split('\\n')\n",
    "            for summarize_sentence in summarize_sentences:\n",
    "                content_set.add(summarize_sentence.strip())\n",
    "        else:\n",
    "            content_set.add(current_summarize)\n",
    "        \n",
    "        content_list = two_page_content.split('\\n')\n",
    "        header = []\n",
    "        for paragraph in content_list:\n",
    "            # split and save paragraph content\n",
    "            if len(paragraph) > text_max_length:\n",
    "                sentence_list = paragraph.split('.')\n",
    "                for sentence in sentence_list:\n",
    "                    content_set.add(sentence)\n",
    "            else:\n",
    "                content_set.add(paragraph.strip())\n",
    "                \n",
    "            # trans table content to json format\n",
    "            if paragraph.find('|') >=0:\n",
    "                if len(header) == 0:\n",
    "                    header = paragraph.split('|')\n",
    "                else:\n",
    "                    content = paragraph.replace('-','').replace(':','').replace('|','')\n",
    "                    if len(content) > 0:\n",
    "                        content = paragraph.split('|')\n",
    "                        line_str = ''\n",
    "                        for i in range(len(header)):\n",
    "                            if i < len(content) and len(str(header[i]).strip()) > 0 and len(str(content[i]).strip()) > 0:\n",
    "                                line_str += (str(header[i]).strip() + ':' + str(content[i]).strip()+ ',')\n",
    "                            elif i < len(content) and len(str(content[i]).strip()) > 0:\n",
    "                                line_str += (str(content[i]).strip()+ ',')\n",
    "                            else:\n",
    "                                line_str += (str(header[i]).strip()+ ',')\n",
    "                        content_set.add(line_str[:-1])\n",
    "\n",
    "            elif paragraph.find('|') < 0 and len(header) > 0:\n",
    "                header = []\n",
    "                \n",
    "        for text in content_set:\n",
    "            text = text.strip()\n",
    "            print('text:',text)\n",
    "            print('--------------')\n",
    "            texts.append(two_page_content)\n",
    "            metadata = {}\n",
    "            metadata['sentence'] = text[:text_max_length] if len(text) > text_max_length else text\n",
    "            metadata['sources'] = file.split('/')[-1]\n",
    "            metadata['page'] = str(i-1) + ' to ' + str(i) if i > 0 else str(i)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "        if len(texts) > 0:\n",
    "            if embedding_type == 'bedrock':\n",
    "                text_embeddings = embeddings.embed_documents([metadata['sentence'] for metadata in metadatas])\n",
    "            else:\n",
    "                text_embeddings = embeddings.embed_documents([metadata['sentence'] for metadata in metadatas],chunk_size=10)\n",
    "\n",
    "            print('texts len:',len(texts))\n",
    "            print('metadatas len:',len(metadatas))\n",
    "            print('embeddings len:',len(text_embeddings))\n",
    "            print('images len:',len(images))\n",
    "            print('begin to save in vectore store')\n",
    "\n",
    "            add_multimodel_documents(\n",
    "                index,\n",
    "                texts=texts,\n",
    "                embeddings=text_embeddings,\n",
    "                metadatas=metadatas,\n",
    "                images=images\n",
    "            )\n",
    "        print('finish save in vectore store:',index)\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        images = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc8e20-7664-4f57-b0e5-baa55259f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
